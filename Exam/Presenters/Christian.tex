\section{Prediction}
\begin{frame}{Prediction}{by Christian}
	\tableofcontents[currentsection, currentsubsection, hideothersubsections]
\end{frame}

\subsection{Introduction to Prediction}
\begin{frame}{Prediction}{Introduction to Prediction}
	\begin{columns}
		\begin{column}{0.5\textwidth}
			\begin{itemize}
				\item Linear frame-based prediction
				\item N is the framelength
				\item O is the overlap between frames
				\item P is the prediction length
				\item M is the filter order
				\item PG is the prediction gain
			\end{itemize}
		\end{column}
		\begin{column}{0.5\textwidth} 
			$PG = 10 log_{10}\bigg(\frac{\sigma^2_x}{\sigma^2_\varepsilon}\bigg)	$
			
			$x$ is the input signal\\
			$\varepsilon$ is the error signal $(x-\hat{x})$
		\end{column}
	\end{columns}
\end{frame}


\begin{frame}{Prediction}{Overview}
	\begin{columns}
		\begin{column}{0.5\textwidth}
			\begin{itemize}
				\item An FIR filter with order M
				\item Filter coefficients are the Linear Prediction Coefficients $\bar{\hat{a}}$
				\item Buffer with length N updated with each N-O new samples
				\item Prediction in cascade 
			\end{itemize}
		\end{column}
		\begin{column}{0.5\textwidth} 
			\begin{figure}
				\includegraphics[width=\textwidth]{figures/WienerHopf}
			\end{figure}
%		$\hat{x}[n+P] =- \sum^{M-1}_{i=1}\hat{a}_i[n]x[(n+P)-i]$
		\end{column}
	\end{columns}
\end{frame}


\subsection{The algorithm}
\begin{frame}{Prediction}{Deriviations}
	\begin{columns}
		\begin{column}{0.45\textwidth}
			\begin{itemize}
				\item The Wiener filter 
				\vspace{3mm}
				\item The LPC found by minimizing 
				\vspace{3mm}
				\item The gradient $\frac{\partial E\{e^2[n]\}}{\partial a_k}$
				\vspace{3mm}
				\item With autocorrelations
				\vspace{3mm}
				\item Yielding Wiener Hopf				
			\end{itemize}
		\end{column}
		\begin{column}{0.55\textwidth} 
			\begin{itemize}
				\item[] $\hat{x}[n+P] =- \sum^{M-1}_{i=1}\hat{a}_i[n]x[(n+P)-i]$
				\vspace{3mm}
				\item[] $E\{e^2[n]\}=E\{(x[n]+\sum_{i=1}^{M}a_ix[n-i] )^2\}$
				\vspace{3mm}
				\item[] $2E\{(x[n]+\sum_{i=1}^{M}a_ix[n-i] )x[n-k]\}=0$
				\vspace{3mm}
				\item[] $r_x[k]+\sum_{i=1}^{M}a_iR_x[i-k]=0$
				\vspace{3mm}
				\item[] $R  \bar{a} = -\bar{r}_x$
			\end{itemize}		
		\end{column}
	\end{columns}
\end{frame}

\subsection{Predicted Speech}
\begin{frame}{Prediction}{Autocorrelation Function Estimation}
	\begin{columns}
		\begin{column}{0.5\textwidth}
			\begin{itemize}
				\item Frame based estimation of autocorrelation function, $\bar{\hat{r}}$
				\item Same  $\bar{\hat{r}}$ used for N-O samples
				\item Non-recursive
				\item Window function
			\end{itemize}
		\end{column}
		\begin{column}{0.5\textwidth} 
			$\hat{r}_x[l] = \sum^{N}_{n=\left| l\right|} x_l[n]w_l[N-n]$\\
			\vspace{5mm}
			Where: \\
			$x_l[n]=x[n]x[n-l]$ and $w_l[n]=w[n]w[n+l]$
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{Prediction}{Solving the Wiener Hopf}
	\begin{columns}
		\begin{column}{0.4\textwidth}
			\begin{itemize}
				\item Wiener Hopf equation $R  \bar{a} = -\bar{r}_x$
				\item By inversion $\bar{a} = -R^{-1} \bar{r}_x$
				\item Levinson Durbin to estimate LPCs 
				\item Three steps using symmetry
			\end{itemize}
			
		\end{column}
		\begin{column}{0.6\textwidth} 
				\resizebox{1.0\columnwidth}{!}{	
				\input{figures/LPC.tex}
			}	
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{Prediction}{A Predicted Signal}
	\begin{columns}
		\begin{column}{0.4\textwidth}
			\begin{itemize}
				\item[\textcolor{MATLABblue}{---}] Speech signal
				\item[\textcolor{MATLABorange}{---}] Predicted speech signal
				\item N = 1200 samples
				\item O = 1100 samples
				\item P = 10 samples
				\item $f_s$ = 48000 Hz
			\end{itemize}
			
		\end{column}
		\begin{column}{0.6\textwidth} 
			\resizebox{1.0\columnwidth}{!}{	
				\input{figures/PredictionTime.tex}
			}
		\end{column}
	\end{columns}
\end{frame}






















%\subsection{Prediction gain}
%\begin{frame}{Prediction}{Prediction gain}
%	\begin{columns}
%		\begin{column}{0.5\textwidth}
%			\begin{itemize}
%			\item Used to quantify the quality of the predictor. 
%			\item $x$ is the input signal
%			\item $\varepsilon$ is the error signal $(x-\hat{x})$
%			\end{itemize}
%		\end{column}
%		\begin{column}{0.5\textwidth} 
%			$PG = 10 log_{10}\bigg(\frac{\sigma^2_x}{\sigma^2_\varepsilon}\bigg)	$
%		\end{column}
%	\end{columns}
%\end{frame}

%\subsection{Determining N,O,M and fs}
%\begin{frame}{Simulation Results}{Optimal parameters P=43}		
%	\begin{columns}
%		\begin{column}{0.4\textwidth}
%			\begin{itemize}
%				\item Prediction order P = 43
%				\item Optimal parameters
%				\begin{itemize}
%					\item Framelength N = 1600
%					\item Overlap O = 1500
%				\end{itemize}
%				\item Prediction Gain PG = 5.4 dB
%			\end{itemize}
%		\end{column}
%		\begin{column}{0.6\textwidth} 
%			\resizebox{0.9\columnwidth}{!}{		
%				\input{figures/HammingNOP43.tex}}
%		\end{column}
%	\end{columns}
%\end{frame}
%
%\begin{frame}{Simulation Results}{Optimal parameters P=10}		
%	\begin{columns}
%		\begin{column}{0.4\textwidth}
%			\begin{itemize}
%				\item Prediction order P = 10
%				\item Optimal parameters
%				\begin{itemize}
%					\item Framelength N = 1200
%					\item Overlap O = 1100
%				\end{itemize}
%				\item Prediction Gain PG = 10 dB
%			\end{itemize}
%		\end{column}
%		\begin{column}{0.6\textwidth} 
%			\resizebox{0.9\columnwidth}{!}{		
%				\input{figures/HammingNOP10.tex}}
%		\end{column}
%	\end{columns}
%\end{frame}
%
%\begin{frame}{Simulation Results}{Optimal parameters fs=12000}		
%	\begin{columns}
%		\begin{column}{0.4\textwidth}
%			\begin{itemize}
%				\item Prediction order P = 3
%				\item Optimal parameters
%				\begin{itemize}
%					\item Framelength N = 300
%					\item Overlap O = 250
%				\end{itemize}
%				\item Prediction Gain PG = 8.5 dB
%			\end{itemize}
%		\end{column}
%		\begin{column}{0.6\textwidth} 
%			\resizebox{0.9\columnwidth}{!}{		
%				\input{figures/HammingNOP3.tex}}
%		\end{column}
%	\end{columns}
%\end{frame}


%\subsection{Prediction}
%\begin{frame}{This is christians part}{some fancy subtitle}		
%	\begin{columns}
%		\begin{column}{0.5\textwidth}
%			fish can be both blue and green	
%		\end{column}
%		\begin{column}{0.5\textwidth} 
%			\begin{itemize}
%				\item<1-> Blue fish
%				\item<2-> Green fish
%				\item<3-> But can we neglect the purple fish?
%				\begin{itemize}
%					\item<4-> Yes, because they are gay
%				\end{itemize}			
%			\end{itemize}		
%		\end{column}
%	\end{columns}
%\end{frame}