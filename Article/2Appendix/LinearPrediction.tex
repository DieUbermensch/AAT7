\section{Linear Prediction}
Linear Prediction (LP) is proposed in order to compensate for the delay introduced by sampling and reconstructing acoustical signals. The delays have high impact on the performance of an ANC system. \\
The proposed predictor uses Wiener filtering to predict the next sample based on the autocorrelation (ACF) of some previous samples. The prediction algorithm is then coupled in cascade in order to predict multiple samples. 

\begin{figure}[H]
	\centering
	\tikzsetnextfilename{WienerHopf}
	\input{figures/ArticleIllustrations/WienerHopf.tex}
	%\includegraphics[width=\columnwidth]{figures/ArticleIllustrations/WienerHopf}
	\caption{Block diagram of linear prediction system.}
	\label{fig:APPLinearPredictionOverview}
\end{figure}
The system predicts $\hat{x}[n+P]$ by adapting linear prediction coefficients $\hat{\bar{a}}$, in a Wiener filter, determined using a framebased Auto Correlation Function (ACF) $\hat{r}_x[l]$ \cite{LinearPrediction}.

\begin{itemize} 
	\item $N$ is the framelength, that is the amount of previous samples stored and used for predicting the next sample. $\bar{x}$ in \autoref{fig:APPLinearPredictionOverview} has size $N$.
	\item $M$ is the order of the predictor, that is the amount of coefficients used for predicting the next sample. $\hat{\bar{a}}$ in \autoref{fig:APPLinearPredictionOverview} has size $M$.
	\item $O$ is the overlap, that is the amount of samples to be reused, when calculating the new ACF. A new ACF and new LPCs are estimated for each $N-O$ samples.
	\item $P$ determines the how many samples will be predicted. 
	\item $W$ is a window used for weighting the samples in the current frame when calculating the ACF. 
\end{itemize}


\subsection{Auto Correlation estimation}
The full autocorrelation would be an infinite time series, which would be impossible to calculate and it would not be useful as speech is not Wide Sense Stationary through infinite time. The short time stationarity of speech is used by only estimating the ACF for a short time frame determined by N/fs. The ACF is estimated using \autoref{eq:Appnonrecursive}.
\begin{equation}\label{eq:Appnonrecursive}
%%r_x[l,m] = \sum^{m}_{n=m-N+1+\left| l\right|} x_l[n]w_l[m-n]
\hat{r}_x[l] = \sum^{N}_{n=\left| l\right|} x_l[n]w_l[N-n]
\end{equation}
Where: l is the lag, $x_l[n]=x[n]x[n-l]$ and $w_l[n]=w[n]w[n+l]$. w is a window that can be applied to avoid discontinuities. Some experimenting has been made using different windows. It was found that a Hamming window yielded best performance, but a rectangular window yielded almost similar results while saving a computations, as multiplying all samples by one can be omitted. A Barnwell window is proposed by \cite{Speech}, but has been found to yield poorer results than both other windows.   
%Where: l is the lag, $x_l[n]=x[n]x[n-l]$ and $w_l[n]=w[n]w[n+l]$. w is a window that can be applied to emphasize specific parts of the signal. A Hamming window emphasizes the middle part of the signal in the middle of the frame. Here a periodicity is reliable, compared to a periodicity at the border of the frame, which might be a random occurrence Some experimenting has been made using different windows. It was found that a Hamming window yielded best performance, but a rectangular window yielded almost similar results while saving a computations, as multiplying all samples by one can be omitted. A Barnwell window is proposed by \cite{Speech}, but has been found to yield poorer results than both other windows.   

\subsection{ Wiener Hopf - Finding the Linear Prediction Coefficients}
 The Linear Prediction Coefficients (LPCs) are determined using \autoref{eq:normal}, known as the Wiener-Hopf equation. The LPC's are found by minimize the mean squared prediction error given as.
 \begin{equation}
 	E\{e^2[n]\}=E\{(x[n]+\sum_{i=1}^{M}a_ix[n-i] )^2\}
 \end{equation}
 The minimum error is found by minimizing the derivative of J with respect to $a_i$ and setting it equal to zero. 
 \begin{equation}\label{eq:minPredError}
 \frac{\partial E\{e^2[n]\}}{\partial a_k} =2E\{(x[n]+\sum_{i=1}^{M}a_ix[n-i] )x[n-k]\}=0
 \end{equation}
 for $k = 1, 2, \cdots, M$. \autoref{eq:minPredError} can be rearranged to yield.
 \begin{equation}
 	E\{x[n]x[n-k]\}+\sum_{i=1}^{M}a_iE\{x[n-i]x[n-k]\}=0
 \end{equation} 
 Or in terms of autocorrelations. 
 \begin{equation}
 R_s[k]+\sum_{i=1}^{M}a_iR_s[i-k]=0
 \end{equation} 
 This can be rewritten and put in matrix form yielding the Wiener-Hopf \autoref{eq:normal}
\begin{equation}\label{eq:normal}
R  \bar{a} = -\bar{r}_x
\end{equation}
Where; $R$ is the covariance matrix $C_{xx}$, $\bar{a}$ is the LPCs $a = [a_0 , a_1, \dots, a_{N-1}]^T$ and $\bar{r}_x$ is the ACF, $\bar{r}_x = [r_x[1] , r_x[2], \dotsc, r_x[N]]^T$. \autoref{eq:normal} can be rewritten as shown in \autoref{eq:normal2} yielding the LPCs directly.  
\begin{equation}\label{eq:normal2}
\bar{a} = -R^{-1} \bar{r}_x
\end{equation}
Calculating $R^{-1}$ is computationally heavy, therefore to estimate the LPCs the Levinson-Durbin method is used. 

\subsubsection{Levinson-Durbin}
Levinson-Durbin is a method which can be used to estimate the LPCs of a problem, recursively. It is characterized by being power efficient, as it does not directly invert a matrix, but rather estimates it.
The algorithm takes the coefficients of the ACF in order to compute the Reflection Coefficients (RCs), which lastly yields the LPCs. The algorithm can be seen below:\\\\
The coefficient order, $l$ is initialized to 0, which means the predictor for order 0 is found first:
\begin{equation}\label{LDInit}
	J_0=R[0]
\end{equation}
The algorithm works recursively, and the next step iterates for a total number of the prediction order, $M$ to compute RCs for each $l$.
\begin{equation}\label{LDStep1}
	k_l=\frac{1}{J_{l-1}} \left ( R[l] + \sum_{i=1}^{l-1} a_i^{(l-1)}R[l-i]   \right) 
\end{equation}
The LPC are then likewise calculated iteratively, from the already found RCs: 
\begin{equation}\label{LDStep2_1}
	a_l^{(l)} = -k_1,
\end{equation}
%\vspace{-10mm}
\begin{equation}\label{LDStep2_2}
a_i^{(l)} = a_i^{(l-1)} - k_l a_{l-i}^{(l-1)} ; \hspace{10mm} i = 1,2,\dotsc,l-1
\end{equation}
Lastly a minimizer for the mean-squared predictor is computed, for the $l$'th order computation:
\begin{equation}\label{LDStep2}
	J_1 = J_{l-1} (1-k_l^2)
\end{equation}
This gives the LPCs:
\begin{equation}
	a_i = a_i^{M}; \hspace{10mm} i = 1,2 \dotsc , M
\end{equation}


\subsection{Wiener filter}
A Wiener filter is used in cascade, where $\hat{x}[n+2]$ is estimated using $\hat{x}[n+1]$ and $x[n]$ up until $\hat{x}[n+P]$. 
\begin{equation}\label{eq:AppPredictor}
\hat{x}[n+p] =- \sum^{M-1}_{i=1}\hat{a}_i[n]x[(n+p)-i]
\end{equation}
The Wiener filter resembles an FIR filter, where the previous inputs are used in combination with the LPCs to predict the next sample. 

\subsection{Prediction Gain}
The Prediction Gain (PG) is an objective measure of the performance of a predictor measured in dB. 
\begin{equation}\label{eq:App_PG}
PG = 10 log_{10}\bigg(\frac{\sigma^2_x}{\sigma^2_\varepsilon}\bigg) = 10 log_{10}\bigg(\frac{E\{x^2[n]\}}{E\{\varepsilon^2[n]\}}\bigg)
\end{equation}
As seen in \autoref{eq:App_PG} the PG is calculated based on a ratio between the variance of the signal to be predicted $\sigma^2_x$ and the variance of the error of the predictor $\sigma^2_\varepsilon$. The PG value yields an estimate of the performance of the predictor, however it is necessary to also listen to the predicted signal, as it might be distorted. PG is used as the performance indicator, while searching for optimum parameters for the predictor.  


%\begin{figure}[H]
%	\tikzsetnextfilename{BasisCompare}
%	\input{../Article/2Appendix/LPFigs/BasisCompare(fig).tex}
%	\label{Fig:BasisCompare}
%	\caption{Text here.}
%\end{figure}

%\begin{figure}[H]
%	\tikzsetnextfilename{LPCompare}
%	\input{../Article/2Appendix/LPFigs/LPCompare(fig).tex}
%	\label{Fig:LPCompare}
%	\caption{Text here.}
%\end{figure}





















